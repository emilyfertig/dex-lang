'# Kernel Regression

import linalg
import stats
import plot

struct ConjGradState(a|VSpace) =
  x : a
  r : a
  p : a

-- Conjugate gradients solver
def solve'(mat:m=>m=>Float, b:m=>Float) -> m=>Float given (m|Ix) =
  x0 = zero
  ax = mat **. x0
  r0 = b - ax
  init = ConjGradState(zero, r0, r0)
  final = fold init \_:m state.
    x = state.x
    r = state.r
    p = state.p
    ap = mat **. p
    alpha = vdot r r / vdot p ap
    x' = x + alpha .* p
    r' = r - alpha .* ap
    beta = vdot r' r' / (vdot r r + 0.000001)
    p' = r' + beta .* p
    ConjGradState(x', r', p')
  final.x

def chol_solve(l:LowerTriMat m Float, b:m=>Float) -> m=>Float given (m|Ix) =
    b' = forward_substitute l b
    u = transpose_lower_to_upper l
    backward_substitute u b'

' # Kernel ridge regression

' To learn a function $f_{true}: \mathcal{X} \to \mathbb R$
from data $(x_1, y_1),\dots,(x_N, y_N)\in \mathcal{X}\times\mathbb R$,\
in kernel ridge regression the hypothesis takes the form
$f(x)=\sum_{i=1}^N \alpha_i k(x_i, x)$,\
where $k:\mathcal X \times \mathcal X \to \mathbb R$ is a positive semidefinite kernel function.\
The optimal coefficients are found by solving a linear system $\alpha=G^{-1}y$,\
where $G_{ij}:=k(x_i, x_j)+\delta_{ij}\lambda$, $\lambda>0$ and $y = (y_1,\dots,y_N)^\top\in\mathbb R^N$

-- Synthetic data
Nx = Fin 100
noise = 0.1
[k1, k2] = split_key (new_key 0)

def trueFun(x:Float) -> Float =
  x + sin (20.0 * x)

xs : Nx=>Float = for i. rand (ixkey k1 i)
ys : Nx=>Float = for i. trueFun xs[i] + noise * randn (ixkey k2 i)

-- Kernel ridge regression
def regress(kernel: (a, a) -> Float, xs: Nx=>a, ys: Nx=>Float) -> (a) -> Float given (a) =
    gram = for i j. kernel xs[i] xs[j] + select (i==j) 0.0001 0.0
    alpha = solve' gram ys
    \x. sum for i. alpha[i] * kernel xs[i] x

def rbf(lengthscale:Float, x:Float, y:Float) -> Float =
  exp (-0.5 * sq ((x - y) / lengthscale))

predict = regress (\x y. rbf 0.2 x y) xs ys

-- Evaluation
Nxtest = Fin 1000
xtest : Nxtest=>Float = for i. rand (ixkey k1 i)
preds = map predict xtest

:html show_plot $ xy_plot xs ys
> <html output>

:html show_plot $ xy_plot xtest preds
> <html output>

' # Gaussian process regression

' GP regression (kriging) works in a similar way. Compared with kernel ridge regression, GP regression assumes Gaussian distributed prior. This, combined
with the Bayes rule, gives the variance of the prediction.

' In this implementation, the conjugate gradient solver is replaced with the
cholesky solver from `lib/linalg.dx` for efficiency.

def gp(
  kernel: (a, a) -> Float,
  xs: n=>a,
  mean_fn: (a) -> Float,
  noise_var: Float
) -> MultivariateNormal n given (n|Ix, a) =
    gram = for i j. kernel xs[i] xs[j]
    loc = for i. mean_fn xs[i]
    chol_cov = chol (gram + eye *. noise_var)
    MultivariateNormal loc chol_cov

def gp_regress_matrix(
  kernel: (a, a) -> Float,
  xs: n=>a,
  mean_fn: (a) -> Float,
  noise_var: Float,
  ys: n=> Float
) -> ((m=>a) -> MultivariateNormal m) given (n|Ix, m|Ix, a) =
    prior_gp = gp kernel xs mean_fn noise_var
    -- How is prior_gp.loc incorporated?
    k_inv_y = chol_solve prior_gp.chol_cov (ys - for i. mean_fn xs[i]) -- n
    predictive_gp_fn = \xs_pred:m=>a.
      k_star = for i j. kernel xs[i] xs_pred[j] -- n by m
      loc = for i. sum for k. k_star[k, i] * k_inv_y[k] + mean_fn xs_pred[i]  -- m
      gram_pred = for i j. kernel xs_pred[i] xs_pred[j]
      s = for i. chol_solve prior_gp.chol_cov (transpose k_star)[i]  -- m by n
      schur = gram_pred - for i j. sum for k. s[i, k] * s[j, k]
      MultivariateNormal loc (chol schur)
    predictive_gp_fn

def gp_regress(
  kernel: (a, a) -> Float,
  xs: n=>a,
  ys: n=>Float
) -> ((a) -> (Float, Float)) given (n|Ix, a) =
    noise_var = 0.0001
    gram = for i j. kernel xs[i] xs[j]
    c = chol (gram + eye *. noise_var)
    alpha = chol_solve c ys
    predict = \x.
        k' = for i. kernel xs[i] x
        mu = sum for i. alpha[i] * k'[i]
        alpha' = chol_solve c k'
        var = kernel x x + noise_var - sum for i. k'[i] * alpha'[i]
        (mu, var)
    predict

gp_predict = gp_regress (\x y. rbf 0.2 x y) xs ys

(gp_preds, vars) = unzip (map gp_predict xtest)

:html show_plot $ xyc_plot xtest gp_preds (map sqrt vars)
> <html output>

:html show_plot $ xy_plot xtest vars
> <html output>

def mean_fn(x:Float) -> Float = 0. * x

N = Fin 10
noise_std = 0.001
[k3, k4, k5] = split_key (new_key 3)
xs_obs : N=>Float = for i. rand (ixkey k3 i)
ys_obs : N=>Float = for i. trueFun xs_obs[i] + noise_std * randn (ixkey k4 i)
M = Fin 40
xs_pred : M=>Float = for i. rand (ixkey k5 i)
gprm_predict : (M=>Float) -> MultivariateNormal M = gp_regress_matrix (\x y. rbf 0.2 x y) xs_obs mean_fn 0.001 ys_obs
pred_dist = gprm_predict xs_pred

